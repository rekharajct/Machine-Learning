{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dx7nSalk2kqj",
        "outputId": "9ea22ba0-d6d4-408a-8396-c9bc4d9e4db7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/train-images-idx3-ubyte.gz\n",
            "Extracting data/MNIST/train-labels-idx1-ubyte.gz\n",
            "Extracting data/MNIST/t10k-images-idx3-ubyte.gz\n",
            "Extracting data/MNIST/t10k-labels-idx1-ubyte.gz\n",
            "Training cost 0.05577536\n",
            "Train accuracy:  100.0\n",
            "Validation accuracy:  98.60000014305115\n",
            "Test accuracy:  98.7500011920929\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import time\n",
        "from datetime import timedelta\n",
        "import math\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "\n",
        "def new_weights(shape):\n",
        "    \"\"\"function to define weights \"\"\"\n",
        "    return tf.Variable(tf.truncated_normal(shape, stddev=0.05))\n",
        "\n",
        "\n",
        "def new_biases(length):\n",
        "    '''function to define bias'''\n",
        "    return tf.Variable(tf.constant(0.05, shape=[length]))\n",
        "\n",
        "\n",
        "def create_new_conv_layer(inp, num_input_channels, filter_size,  num_filters): \n",
        "    '''\n",
        "    function to create new convolutional layers\n",
        "    input: input image, number of input channels, filter size, number of filters\n",
        "    output: convolution layer, weights\n",
        "    '''\n",
        "    #shape of the filter    \n",
        "    shape = [filter_size, filter_size, num_input_channels, num_filters]\n",
        "\n",
        "    #create new weights\n",
        "    weights = new_weights(shape)\n",
        "\n",
        "    #create new biases, one for each filter.\n",
        "    biases = new_biases(num_filters)\n",
        "    \n",
        "    #create convolution layer\n",
        "    layer = tf.nn.conv2d(input=inp,\n",
        "                         filter=weights,\n",
        "                         strides=[1, 1, 1, 1],\n",
        "                         padding='SAME')\n",
        "    #add biases \n",
        "    layer = layer+biases\n",
        "    \n",
        "    #Max pooling of size 2x2 with a stride of 2.Â \n",
        "    layer = tf.nn.max_pool(value=layer,\n",
        "                               ksize=[1, 2, 2, 1],\n",
        "                               strides=[1, 2, 2, 1],\n",
        "                               padding='SAME')\n",
        "    \n",
        "\n",
        "    # use non-Linearity used in all layers,  ReLU \n",
        "    layer = tf.nn.relu(layer)\n",
        "\n",
        "    return layer, weights\n",
        "\n",
        "\n",
        "def flatten_layer(layer):\n",
        "    '''\n",
        "    Function to flatten the convolution layer\n",
        "    inputs: convolution layer\n",
        "    output: flattened layer, number of features\n",
        "    '''\n",
        "    # shape of the input layer.\n",
        "    # layer_shape == [num_images, img_height, img_width, num_channels]\n",
        "    layer_shape = layer.get_shape()\n",
        "\n",
        "    # get number of features\n",
        "    # The number of features is: img_height * img_width * num_channels\n",
        "    num_features = layer_shape[1:4].num_elements()\n",
        "    \n",
        "    # reshape the layer to [num_images, num_features].\n",
        "    # The shape of the flattened layer is  [num_images, img_height * img_width * num_channels]\n",
        "    layer_flat = tf.reshape(layer, [-1, num_features])\n",
        "\n",
        "    \n",
        "    return layer_flat, num_features\n",
        "\n",
        "\n",
        "def create_new_fully_conn_layer(inp,  num_inputs, num_outputs): \n",
        "    '''\n",
        "    Function to create fully connected layer\n",
        "    input: previous layer\n",
        "    output: fully connected layer\n",
        "    '''\n",
        "    \n",
        "    # Create new weights and biases.\n",
        "    weights = new_weights([num_inputs, num_outputs])\n",
        "    biases = new_biases(num_outputs)\n",
        "\n",
        "    # create the layer by multiplying weights and inputs and adding bias\n",
        "    layer = tf.matmul(inp, weights) + biases\n",
        "\n",
        "    # use ReLu\n",
        "    layer = tf.nn.relu(layer)\n",
        "    \n",
        "    return layer\n",
        "\n",
        "\n",
        "def create_new_softmax_layer(inp,  num_inputs, num_outputs): \n",
        "    '''\n",
        "    Function to create softmax_layer layer\n",
        "    input: fully connected layer\n",
        "    output: layer, softmax predictions\n",
        "    '''\n",
        "    \n",
        "    # Create new weights and biases.\n",
        "    weights = new_weights([num_inputs, num_outputs])\n",
        "    biases = new_biases(num_outputs)\n",
        "\n",
        "    # create the layer by multiplying weights and inputs and adding bias\n",
        "    layer = tf.matmul(inp, weights) + biases\n",
        "    pred = tf.nn.softmax(layer)\n",
        "    \n",
        "    \n",
        "    return layer,pred\n",
        "\n",
        "\n",
        "def cnn_mnist(learning_rate,iterations):  \n",
        "    '''\n",
        "    Function of CNN model\n",
        "    '''\n",
        "        \n",
        "    #configuration of convolution layers\n",
        "\n",
        "    \n",
        "    # Convolutional Layer 1.\n",
        "    filter_size1 = 5          # each filter 5x5 pixel\n",
        "    num_filters1 = 6         # 6 filters\n",
        "\n",
        "    # Convolutional Layer 2.\n",
        "    filter_size2 = 5          # each filter 5X5 pixel\n",
        "    num_filters2 = 16         # 16 filters\n",
        "\n",
        "    # Fully-connected layer1.\n",
        "    fully_conn_layer_size1 = 120            # Number of units in the fully connected hidden layer1 = 120\n",
        "\n",
        "    # Fully-connected layer2.\n",
        "    fully_conn_layer_size2 = 84            # Number of units in the fully connected hidden layer2 = 84\n",
        "\n",
        "    #load mnist image\n",
        "    data = input_data.read_data_sets('data/MNIST/', one_hot=True)\n",
        "    \n",
        "    \n",
        "    #class labels of test data\n",
        "    Y_test = np.argmax(data.test.labels, axis=1)\n",
        "\n",
        "    #data dimensions\n",
        "    img_size = 28\n",
        "\n",
        "    # flattened image\n",
        "    img_size_flat = img_size * img_size\n",
        "\n",
        "    # shape of the image\n",
        "    img_shape = (img_size, img_size)\n",
        "\n",
        "    # Number of colour channels for the images: 1 channel for gray-scale.\n",
        "    num_channels = 1\n",
        "\n",
        "    # Number of classes, one class for each of 10 digits.\n",
        "    num_classes = 10\n",
        "\n",
        "    #Placeholders\n",
        "    x = tf.placeholder(tf.float32, shape=[None, img_size_flat])\n",
        "    #reshape the image\n",
        "    x_image = tf.reshape(x, [-1, img_size, img_size, num_channels])\n",
        "\n",
        "    #true y in one hot vector form\n",
        "    y_true_one_hot = tf.placeholder(tf.float32, shape=[None, num_classes])\n",
        "\n",
        "    #classes in y\n",
        "    y_true_class = tf.argmax(y_true_one_hot, axis=1)\n",
        "\n",
        "    #create first convolution layer with input as image\n",
        "    conv_layer1, weights_conv1 = create_new_conv_layer(x_image, num_channels,\n",
        "                                                       filter_size1, num_filters1)\n",
        "\n",
        "    #create second convolution layer with input as the output from first convolution layer \n",
        "    #number of input channels= number of filters in first layer\n",
        "    conv_layer2, weights_conv2 = create_new_conv_layer(conv_layer1, num_filters1, filter_size2,num_filters2)\n",
        "\n",
        "    #flatten the layers\n",
        "    layer_flat, num_features = flatten_layer(conv_layer2)\n",
        "    #create fully connected layer1 with the flattened layer as input\n",
        "    fully_conn_layer1 = create_new_fully_conn_layer(layer_flat,num_features,fully_conn_layer_size1)\n",
        "    #create fully connected layer2 with the  output of fully connected layer1 as input\n",
        "    fully_conn_layer2 = create_new_fully_conn_layer(fully_conn_layer1,fully_conn_layer_size1,fully_conn_layer_size2)\n",
        "    #create softmax layer\n",
        "    soft_layer, y_pred = create_new_softmax_layer(fully_conn_layer2, fully_conn_layer_size2,num_classes)\n",
        "\n",
        "    #find the predicted classes\n",
        "    #y_pred = tf.nn.softmax(fully_conn_layer2)\n",
        "    y_pred_class = tf.argmax(y_pred, axis=1)\n",
        "\n",
        "    #optimize cost function\n",
        "    cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=soft_layer,labels=y_true_one_hot)\n",
        "   \n",
        "\n",
        "    cost = tf.reduce_mean(cross_entropy)\n",
        "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        "\n",
        "    #find accuracy\n",
        "    correct_prediction = tf.equal(y_pred_class, y_true_class)\n",
        "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "    #tensorflow run\n",
        "    session = tf.Session()\n",
        "    session.run(tf.global_variables_initializer())\n",
        "\n",
        "    #train the cnn\n",
        "    train_batch_size=100\n",
        "    for i in range(iterations):\n",
        "        #get a batch of training data\n",
        "        x_batch, y_true_batch = data.train.next_batch(train_batch_size)\n",
        "\n",
        "        res=session.run([cost,optimizer], feed_dict={x:x_batch, y_true_one_hot:y_true_batch})\n",
        "        acc=session.run(accuracy, feed_dict={x:x_batch, y_true_one_hot:y_true_batch})\n",
        "    print(\"Training cost\",res[0])\n",
        "    print(\"Train accuracy: \",acc*100)\n",
        "    \n",
        "    #validate the cnn\n",
        "    acc=session.run(accuracy, feed_dict={x:data.validation.images , y_true_one_hot:data.validation.labels})\n",
        "   \n",
        "    print(\"Validation accuracy: \",acc*100)\n",
        "    \n",
        "    #test the cnn\n",
        "    acc=session.run(accuracy, feed_dict={x:data.test.images , y_true_one_hot:data.test.labels})\n",
        "    \n",
        "    print(\"Test accuracy: \",acc*100)\n",
        "    \n",
        "    \n",
        "\n",
        "cnn_mnist(0.2,2000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zGC_nPR72kqq"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMI1QMex2kqq"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    },
    "colab": {
      "name": "assignment4.ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}